{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "projects = [\n",
    "    'DigiBatMat',\n",
    "    'DIGITRUBBER',\n",
    "    'DiProMag',\n",
    "    #'DiStAl',\n",
    "    'GlasDigital',\n",
    "    #'iBain',\n",
    "    'KNOW-NOW',\n",
    "    'KupferDigital',\n",
    "    'LeBeDigital',\n",
    "    'ODE_AM',\n",
    "    'PMDao_MO',\n",
    "    'PMDao_TTO',\n",
    "    'SensoTwin',\n",
    "    'SmaDi',\n",
    "    'StahlDigital'\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "for ont in projects:\n",
    "    with open(f'{ont}/{ont}.json', 'r', encoding='utf-8') as f:\n",
    "        data.update({ont: json.load(f)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Top-Level-Ontologies\n",
    "For each of the provided ontologies the use of TLOs was analyzed. This was achieved by counting rdfs:subClassOf and rdfs:subPropertyOf chains, for which the subject belongs to the projects namespace and the object belongs to the TLOs namespace. For example, the SPARQL-Query for the usage of PMD Core Ontology (v2.0.x) in the SensoTwin project reads:\n",
    "```sparql\n",
    "SELECT (COUNT(*) as ?subcount)\n",
    "WHERE {\n",
    "    ?ao rdfs:subClassOf+|rdfs:subPropertyOf+ ?tlo .\n",
    "    FILTER( STRSTARTS( STR(?tlo), \"https://w3id.org/pmd/co\" ) ) .\n",
    "    FILTER( STRSTARTS( STR(?ao), \"http://w3id.org/sensotwin/applicationontology\" ) ) .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ont: {tlo: d['subclassesproperties'] for tlo, d in item['tlos']['reasoned'].items()} for ont, item in data.items()}).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sparql\n",
    "SELECT (COUNT(*) as ?count)\n",
    "WHERE {\n",
    "    ?ao ?p ?o .\n",
    "    FILTER( STRSTARTS( STR(?p), \"https://w3id.org/pmd/co\" ) ) .\n",
    "    FILTER( STRSTARTS( STR(?ao), \"http://w3id.org/sensotwin/applicationontology\" ) ) .\n",
    "}\n",
    "```\n",
    "\n",
    "```sparql\n",
    "SELECT (COUNT(*) as ?count)\n",
    "WHERE {\n",
    "    ?ao ?p ?o .\n",
    "    FILTER( STRSTARTS( STR(?o), \"https://w3id.org/pmd/co\" ) ) .\n",
    "    FILTER( STRSTARTS( STR(?ao), \"http://w3id.org/sensotwin/applicationontology\" ) ) .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ont: {tlo: d['objects']+d['predicates'] for tlo, d in item['tlos']['reasoned'].items()} for ont, item in data.items()}).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall defined concepts\n",
    "The overall number of introduced concepts was analysed. For that, the projects ontology as well as the applicable pmdco were loaded into Protégé and a Reasoner was run. On the resultant graph, the following query was executed (exemplary for `owl:Class`es in SensoTwin):\n",
    "\n",
    "```sparql\n",
    "SELECT (COUNT(*) as ?classcount)\n",
    "WHERE {\n",
    "    ?class a owl:Class .\n",
    "    FILTER STRSTARTS( ?class, \"http://w3id.org/sensotwin/applicationontology\" ) .\n",
    "}\n",
    "```\n",
    "\n",
    "The table below shows the respective numbers of found definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = {ont: {\n",
    "    'owl:Class': item['definitioncounts']['owl:Class'],\n",
    "    'owl:ObjectProperty': item['definitioncounts']['owl:ObjectProperty'],\n",
    "    'owl:DatatypeProperty': item['definitioncounts']['owl:DatatypeProperty'],\n",
    "    'Total': item['definitioncounts']['owl:Class']+item['definitioncounts']['owl:ObjectProperty']+item['definitioncounts']['owl:DatatypeProperty'],\n",
    "    'Reasoner': f\"{item['reasoner']['reasoner']}-{item['reasoner']['version']}\"\n",
    "} for ont, item in data.items()}\n",
    "pd.DataFrame(concepts).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of ProcessingNodes, ValueObjects, Processes, Objects (pmdco-2.0.x) and ProcessNodes (pmdco-v0.1-beta)\n",
    "To get an overview over the usage of the PMD Core Ontology the number of subclasses of ProcessingNode, ValueObject, Process and Object was determined. For that, the projects ontology as well as the applicable pmdco were loaded into Protégé and a Reasoner was run. On the resultant graph, the following query was executed (exemplary for sub-classes of ProcessingNode in SensoTwin):\n",
    "\n",
    "```sparql\n",
    "SELECT ?classname\n",
    "WHERE {\n",
    "    ?x rdfs:subClassOf+ <https://w3id.org/pmd/co/ProcessingNode> .\n",
    "    BIND(STR(?x) AS ?classname) .\n",
    "    FILTER STRSTARTS( ?classname, \"http://w3id.org/sensotwin/applicationontology\" ) .\n",
    "}\n",
    "```\n",
    "\n",
    "The table below shows the respective numbers of found definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmdusage = {ont: {\n",
    "    'ProcessingNode (2.0.x)': item['processingnodes']['pmdco-2.0.7']['count'],\n",
    "    'ValueObject (2.0.x)': item['valueobjects']['pmdco-2.0.7']['count'],\n",
    "    'Process (2.0.x)': item['processes']['pmdco-2.0.7']['count'],\n",
    "    'Object (2.0.x)': item['objects']['pmdco-2.0.7']['count'],\n",
    "    'Total (2.0.x)': item['processingnodes']['pmdco-2.0.7']['count']+item['valueobjects']['pmdco-2.0.7']['count']+item['processes']['pmdco-2.0.7']['count']+item['objects']['pmdco-2.0.7']['count'],\n",
    "    'ProcessNode (v0.1-beta)': item['processingnodes']['pmdco-v0.1-beta']['count'],\n",
    "    'Total (v0.1-beta)': item['processingnodes']['pmdco-v0.1-beta']['count'],\n",
    "    'Total (both)': item['processingnodes']['pmdco-2.0.7']['count']+item['valueobjects']['pmdco-2.0.7']['count']+item['processes']['pmdco-2.0.7']['count']+item['objects']['pmdco-2.0.7']['count']+item['processingnodes']['pmdco-v0.1-beta']['count'],\n",
    "    'Reasoner': f\"{item['reasoner']['reasoner']}-{item['reasoner']['version']}\"\n",
    "} for ont, item in data.items()}\n",
    "pd.DataFrame(pmdusage).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Licenses\n",
    "The following table summarizes the referenced licenses. The SPARQL used for finding this information reads:\n",
    "```sparql\n",
    "SELECT ?lic\n",
    "WHERE {\n",
    "    ?x <http://purl.org/dc/terms/license>|<http://purl.org/dc/elements/1.1/license> ?lic .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def license_cleanup(license):\n",
    "    replacements = [\n",
    "        ('https://creativecommons.org/licenses/by/4.0', 'CC BY 4.0'),\n",
    "        ('http://creativecommons.org/licenses/by/4.0', 'CC BY 4.0'),\n",
    "        ('https://creativecommons.org/licenses/by-sa/4.0/', 'CC BY-SA 4.0'),\n",
    "        ('https://creativecommons.org/licenses/unspecified', '')\n",
    "    ]\n",
    "    license = license.replace('<', '').replace('>', '')\n",
    "    for old, new in replacements:\n",
    "        if license.startswith(old):\n",
    "            return new\n",
    "    return license\n",
    "\n",
    "licenses = {ont: {'used_licenses': ', '.join(map(license_cleanup, set(item['license']['items'])))} for ont, item in data.items()}\n",
    "pd.DataFrame(licenses).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import rdflib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def pp(df):\n",
    "    return display(HTML(df.to_html().replace('\\\\n', '<br>')))\n",
    "\n",
    "def orcid_resolve(string):\n",
    "    m = re.match(r\"<?(https://orcid.org/(\\d{4}-\\d{4}-\\d{4}-\\d{3}[\\dX]))>?\", string)\n",
    "    if m:\n",
    "        orcid = m.group(1)\n",
    "        stype = 'uri' if f'<{orcid}>' == string else 'literal'\n",
    "\n",
    "        g = rdflib.Graph()\n",
    "        g.parse(orcid)\n",
    "        names = []\n",
    "        [names.append(str(row.gname)) for row in g.query(\n",
    "            f\"\"\"\n",
    "                SELECT ?gname WHERE {{\n",
    "                    <{orcid}> <http://xmlns.com/foaf/0.1/givenName> ?gname .\n",
    "                }}\n",
    "            \"\"\"\n",
    "        )]\n",
    "        [names.append(str(row.fname)) for row in g.query(\n",
    "            f\"\"\"\n",
    "                SELECT ?fname WHERE {{\n",
    "                    <{orcid}> <http://xmlns.com/foaf/0.1/familyName> ?fname .\n",
    "                }}\n",
    "            \"\"\"\n",
    "        )]\n",
    "        name = ' '.join(names)\n",
    "        return f'{orcid} ({stype}) -> {name}'\n",
    "    return string\n",
    "\n",
    "contributors = {ont: {'creators_contributors': '\\n'.join(map(orcid_resolve, set(item['creators_contributors']['items'])))} for ont, item in data.items()}\n",
    "df = pd.DataFrame(contributors).T\n",
    "pp(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespaces\n",
    "To analyze which namespaces were used in the projects T-Boxes, the ontology files were parsed for all occurences of semantically valid uris (`'<(https?:\\/\\/([0-9A-z-_~\\.]+[\\/|#])+)'`). The list of uris was stored (`requests_raw.xlsx`) and manually curated with applicable namespace identifiers (`requests.xlsx`). This approach was necessary, as in some of the ontology files wrong or ambiguous identifiers were used (e.g. `http://material-digital.de/pmdco/` instead of `https://material-digital.de/pmdco/`). For all uris it was tested, if they are dereferecenceable. If so, it was checked if they allow for content negotiation and deliver some owl serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "mime_types = ['text/turtle','application/rdf+xml','application/ld+json','application/n-triples']\n",
    "res = {\n",
    "    'accept': {},\n",
    "    'noaccept': {},\n",
    "    'error': {}\n",
    "}\n",
    "\n",
    "all_namespaces = list(set(x for ds in data.values() for x in ds['namespaces']['items']))\n",
    "\n",
    "f = IntProgress(min=0, max=len(all_namespaces))\n",
    "display(f)\n",
    "\n",
    "for x in all_namespaces:\n",
    "    f.value += 1\n",
    "    try:\n",
    "        req = requests.head(x, headers={'Accept': ','.join(mime_types)}, allow_redirects=True)\n",
    "        if req.headers['content-type'] in mime_types: \n",
    "            res['accept'].update({x: {'status_code': req.status_code, 'content_type': req.headers['content-type']}})\n",
    "        else:\n",
    "            res['noaccept'].update({x: {'status_code': req.status_code, 'content_type': req.headers['content-type']}})\n",
    "    except Exception as e:\n",
    "        res['error'].update({x: {'error': e}})\n",
    "responses = pd.concat((\n",
    "    pd.DataFrame(res['accept']).T, \n",
    "    pd.DataFrame(res['noaccept']).T, \n",
    "    #pd.DataFrame(res['error']).T\n",
    "))\n",
    "\n",
    "import json\n",
    "with open('requests.json', 'r', encoding='utf8') as jf:\n",
    "    requests_data = json.load(jf)\n",
    "\n",
    "for a, x in responses.iterrows():\n",
    "    if a not in requests_data:\n",
    "        requests_data.update({a: {\n",
    "            'status_code': x['status_code'],\n",
    "            'content_type': x['content_type'],\n",
    "            'countas_tlo': None,\n",
    "            'countas_ao': None}})\n",
    "        print(f'added {a} to requests_data (with empty information)')\n",
    "    if a in requests_data:\n",
    "        if not x['status_code'] == requests_data[a]['status_code']:\n",
    "            requests_data[a].update({'status_code': x['status_code']})\n",
    "            print(f'updated status code for {a}')\n",
    "        if not x['content_type'] == requests_data[a]['content_type']:\n",
    "            requests_data[a].update({'content_type': x['content_type']})\n",
    "            print(f'updated content type for {a}')\n",
    "\n",
    "with open('requests.json', 'w', encoding='utf8') as jf:\n",
    "    json.dump(requests_data, jf, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespace identifiers\n",
    "The following uri(-stubs) were collected into the respective namespace identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requests.json', 'r', encoding='utf8') as jf:\n",
    "    requests_data = json.load(jf)\n",
    "\n",
    "tlodict = dict()\n",
    "for tkey, tval in requests_data.items():\n",
    "    if not tval['countas_tlo']:\n",
    "        continue\n",
    "    if tval['countas_tlo'] not in tlodict:\n",
    "        tlodict[tval['countas_tlo']] = {'uris': [tkey]}\n",
    "    else:\n",
    "        tlodict[tval['countas_tlo']]['uris'].append(tkey)\n",
    "tlodict = {k: {'uris': ', '.join(v['uris'])} for k, v in tlodict.items()}\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(tlodict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodict = dict()\n",
    "for tkey, tval in requests_data.items():\n",
    "    if not tval['countas_ao']:\n",
    "        continue\n",
    "    if tval['countas_ao'] not in aodict:\n",
    "        aodict[tval['countas_ao']] = {'uris': [tkey]}\n",
    "    else:\n",
    "        aodict[tval['countas_ao']]['uris'].append(tkey)\n",
    "aodict = {k: {'uris': ', '.join(v['uris'])} for k, v in aodict.items()}\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(aodict).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLO usage\n",
    "The used TLOs are listed in the table below. Also trivial cases like `owl` and `rdfs` were included. The column `Sum` denoted the number of evaluated A-Boxes, that used concepts belonging to the respective namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')\n",
    "tlodict = dict()\n",
    "for tkey, tval in requests_data.items():\n",
    "    if not tval['countas_tlo']:\n",
    "        continue\n",
    "    if tval['countas_tlo'] not in tlodict:\n",
    "        tlodict[tval['countas_tlo']] = {'uris': [tkey]}\n",
    "    else:\n",
    "        tlodict[tval['countas_tlo']]['uris'].append(tkey)\n",
    "\n",
    "dftlo = pd.DataFrame({proj: {key: max([int(x in data[proj]['namespaces']['items']) for x in tlodict[key]['uris']]) for key in tlodict.keys()} for proj in data.keys()})\n",
    "dftlo.insert(loc=len(dftlo.columns), column='Sum', value=dftlo.sum(axis=1))\n",
    "dftlo['name'] = dftlo.index\n",
    "dftlo.sort_values(by=['Sum', 'name'], ascending=[False, True], inplace=True)\n",
    "dftlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-AO usage\n",
    "The table below denotes the usage of concepts from other projects namespaces. It can easily be seen, that there is no concept usage between the projects visible in the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aodict = dict()\n",
    "for tkey, tval in requests_data.items():\n",
    "    if not tval['countas_ao']:\n",
    "        continue\n",
    "    if tval['countas_tlo'] not in aodict:\n",
    "        aodict[tval['countas_ao']] = {'uris': [tkey]}\n",
    "    else:\n",
    "        aodict[tval['countas_ao']]['uris'].append(tkey)\n",
    "\n",
    "dfao = pd.DataFrame({proj: {key: max([int(x in data[proj]['namespaces']['items']) for x in aodict[key]['uris']]) for key in aodict.keys()} for proj in data.keys()})\n",
    "dfao.insert(loc=len(dfao.columns), column='Sum', value=dfao.sum(axis=1))\n",
    "dfao['name'] = dfao.index\n",
    "dfao.sort_values(by=['Sum', 'name'], ascending=[False, True], inplace=True)\n",
    "dfao[list(dfao.index) + ['Sum']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmd_kernel",
   "language": "python",
   "name": "pmd_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d1d5feec15dbfe738666fd10cea7256000e8b4477dd9d92c4285180248ab7ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
